{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9752855,"sourceType":"datasetVersion","datasetId":5971331}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ“Š Daily Sales Report â€” Google ADK Capstone\n#### Multi-agent System, Data Pipeline Development, Agent Tools for a Daily Sales Report System Using Gemini + Google ADK Technologies","metadata":{}},{"cell_type":"markdown","source":"## Project Summary\nThis project implementation includes the following:\n- A data cleaning pipeline for an online sales dataset\n- A daily-report generator (metrics, anomalies, trends)\n- AI agents, agent tools, and orchestration workflow (daily_report_agent, summary_agent, followup_agent)\n- Local testing utilities and a Streamlit front-end prototype\n\nIntended use: local development and testing before production deployment.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:38:55.055076Z","iopub.execute_input":"2025-11-24T18:38:55.055418Z","iopub.status.idle":"2025-11-24T18:38:55.547046Z","shell.execute_reply.started":"2025-11-24T18:38:55.055389Z","shell.execute_reply":"2025-11-24T18:38:55.545729Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/online-sales-dataset/online_sales_dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:38:55.549352Z","iopub.execute_input":"2025-11-24T18:38:55.550154Z","iopub.status.idle":"2025-11-24T18:38:55.625092Z","shell.execute_reply.started":"2025-11-24T18:38:55.550108Z","shell.execute_reply":"2025-11-24T18:38:55.624156Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Setup and authentication complete.\")\nexcept Exception as e:\n    print(\n        f\"ðŸ”‘ Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:38:55.626180Z","iopub.execute_input":"2025-11-24T18:38:55.626509Z","iopub.status.idle":"2025-11-24T18:38:55.681825Z","shell.execute_reply.started":"2025-11-24T18:38:55.626478Z","shell.execute_reply":"2025-11-24T18:38:55.680327Z"}},"outputs":[{"name":"stdout","text":"âœ… Setup and authentication complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Configurations and Environments\n\n* Environment check\n* This cell verifies package versions and provides packages required for this to run.","metadata":{}},{"cell_type":"code","source":"# Import libraries for this notebook\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport decimal\n\nfrom google.genai import types\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner, Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import AgentTool, ToolContext\n\nfrom typing import Any, Dict\n\nfrom google.adk.apps.app import App, EventsCompactionConfig\n\nprint(\"âœ… ADK components imported successfully.\")\nprint(\"âœ… Libraries installed and the dataset has been loaded too.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:38:55.682929Z","iopub.execute_input":"2025-11-24T18:38:55.683356Z","iopub.status.idle":"2025-11-24T18:39:21.228382Z","shell.execute_reply.started":"2025-11-24T18:38:55.683318Z","shell.execute_reply":"2025-11-24T18:39:21.227297Z"}},"outputs":[{"name":"stdout","text":"âœ… ADK components imported successfully.\nâœ… Libraries installed and the dataset has been loaded too.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"APP_NAME = \"default\"  # Application\nUSER_ID = \"default\"  # User\nSESSION = \"default\"  # Session\n\nMODEL_NAME = \"gemini-2.5-flash-lite\"\n\n# Set up Session Management\n# InMemorySessionService stores conversations in RAM (temporary)\nsession_service = InMemorySessionService()\n\nprint(\"âœ… Session state created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.231510Z","iopub.execute_input":"2025-11-24T18:39:21.232111Z","iopub.status.idle":"2025-11-24T18:39:21.238266Z","shell.execute_reply.started":"2025-11-24T18:39:21.232083Z","shell.execute_reply":"2025-11-24T18:39:21.237051Z"}},"outputs":[{"name":"stdout","text":"âœ… Session state created.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Data Loading & Exploration Process","metadata":{}},{"cell_type":"markdown","source":"#### The dataset was taken from Kaggle data store\n\nThe dataset comprises anonymized data on online sales transactions, capturing various aspects of product purchases, customer details, and order characteristics.\n\n**Content of the dataset**\n\n| Column Name        | Description                                                                 |\n|--------------------|-----------------------------------------------------------------------------|\n| InvoiceNo          | A unique identifier for each sales transaction (invoice).                   |\n| StockCode          | The code representing the product stock-keeping unit (SKU).                 |\n| Description        | A brief description of the product.                                         |\n| Quantity           | The number of units of the product sold in the transaction.                 |\n| InvoiceDate        | The date and time when the sale was recorded.                               |\n| UnitPrice          | The price per unit of the product in the transaction currency.             |\n| CustomerID         | A unique identifier for each customer.                                      |\n| Country            | The customer's country.                                                     |\n| Discount           | The discount applied to the transaction, if any.                            |\n| PaymentMethod      | The method of payment used for the transaction (e.g. PayPal, Bank Transfer).|\n| ShippingCost       | The cost of shipping for the transaction.                                   |\n| Category           | The category to which the product belongs (e.g. Electronics, Apparel).      |\n| SalesChannel       | The channel through which the sale was made (e.g. Online, In-store).        |\n| ReturnStatus       | Indicates whether the item was returned or not.                             |\n| ShipmentProvider   | The provider responsible for delivering the order (e.g. UPS, FedEx).        |\n| WarehouseLocation  | The warehouse location from which the order was fulfilled.                  |\n| OrderPriority      | The priority level of the order (e.g. High, Medium, Low).                   |\n\nFor more details: https://www.kaggle.com/datasets/yusufdelikkaya/online-sales-dataset","metadata":{}},{"cell_type":"code","source":"# Load dataset and inspect it\n\n# -------- LOAD DATASETS --------\ncsv_path = \"/kaggle/input/online-sales-dataset/online_sales_dataset.csv\"\n# ------------------------\n\n#load for preview\ndf = pd.read_csv(csv_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#data inspection process - check the structure of the dataset\ndetails = df.info()\ndetails","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.450932Z","iopub.execute_input":"2025-11-24T18:39:21.451255Z","iopub.status.idle":"2025-11-24T18:39:21.496012Z","shell.execute_reply.started":"2025-11-24T18:39:21.451226Z","shell.execute_reply":"2025-11-24T18:39:21.494634Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 49782 entries, 0 to 49781\nData columns (total 17 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   InvoiceNo          49782 non-null  int64  \n 1   StockCode          49782 non-null  object \n 2   Description        49782 non-null  object \n 3   Quantity           49782 non-null  int64  \n 4   InvoiceDate        49782 non-null  object \n 5   UnitPrice          49782 non-null  float64\n 6   CustomerID         44804 non-null  float64\n 7   Country            49782 non-null  object \n 8   Discount           49782 non-null  float64\n 9   PaymentMethod      49782 non-null  object \n 10  ShippingCost       47293 non-null  float64\n 11  Category           49782 non-null  object \n 12  SalesChannel       49782 non-null  object \n 13  ReturnStatus       49782 non-null  object \n 14  ShipmentProvider   49782 non-null  object \n 15  WarehouseLocation  46297 non-null  object \n 16  OrderPriority      49782 non-null  object \ndtypes: float64(4), int64(2), object(11)\nmemory usage: 6.5+ MB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#data inspection process - check samples of the contents inside the dataset\npreview = df.head()\npreview","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.497218Z","iopub.execute_input":"2025-11-24T18:39:21.497575Z","iopub.status.idle":"2025-11-24T18:39:21.523870Z","shell.execute_reply.started":"2025-11-24T18:39:21.497531Z","shell.execute_reply":"2025-11-24T18:39:21.522347Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   InvoiceNo StockCode Description  Quantity       InvoiceDate  UnitPrice  \\\n0     221958  SKU_1964   White Mug        38  2020-01-01 00:00       1.71   \n1     771155  SKU_1241   White Mug        18  2020-01-01 01:00      41.25   \n2     231932  SKU_1501  Headphones        49  2020-01-01 02:00      29.11   \n3     465838  SKU_1760   Desk Lamp        14  2020-01-01 03:00      76.68   \n4     359178  SKU_1386   USB Cable       -30  2020-01-01 04:00     -68.11   \n\n   CustomerID         Country  Discount  PaymentMethod  ShippingCost  \\\n0     37039.0       Australia  0.470000  Bank Transfer         10.79   \n1     19144.0           Spain  0.190000        paypall          9.51   \n2     50472.0         Germany  0.350000  Bank Transfer         23.03   \n3     96586.0     Netherlands  0.140000        paypall         11.08   \n4         NaN  United Kingdom  1.501433  Bank Transfer           NaN   \n\n      Category SalesChannel  ReturnStatus ShipmentProvider WarehouseLocation  \\\n0      Apparel     In-store  Not Returned              UPS            London   \n1  Electronics       Online  Not Returned              UPS              Rome   \n2  Electronics       Online      Returned              UPS            Berlin   \n3  Accessories       Online  Not Returned       Royal Mail              Rome   \n4  Electronics     In-store  Not Returned            FedEx               NaN   \n\n  OrderPriority  \n0        Medium  \n1        Medium  \n2          High  \n3           Low  \n4        Medium  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceNo</th>\n      <th>StockCode</th>\n      <th>Description</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>UnitPrice</th>\n      <th>CustomerID</th>\n      <th>Country</th>\n      <th>Discount</th>\n      <th>PaymentMethod</th>\n      <th>ShippingCost</th>\n      <th>Category</th>\n      <th>SalesChannel</th>\n      <th>ReturnStatus</th>\n      <th>ShipmentProvider</th>\n      <th>WarehouseLocation</th>\n      <th>OrderPriority</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>221958</td>\n      <td>SKU_1964</td>\n      <td>White Mug</td>\n      <td>38</td>\n      <td>2020-01-01 00:00</td>\n      <td>1.71</td>\n      <td>37039.0</td>\n      <td>Australia</td>\n      <td>0.470000</td>\n      <td>Bank Transfer</td>\n      <td>10.79</td>\n      <td>Apparel</td>\n      <td>In-store</td>\n      <td>Not Returned</td>\n      <td>UPS</td>\n      <td>London</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>771155</td>\n      <td>SKU_1241</td>\n      <td>White Mug</td>\n      <td>18</td>\n      <td>2020-01-01 01:00</td>\n      <td>41.25</td>\n      <td>19144.0</td>\n      <td>Spain</td>\n      <td>0.190000</td>\n      <td>paypall</td>\n      <td>9.51</td>\n      <td>Electronics</td>\n      <td>Online</td>\n      <td>Not Returned</td>\n      <td>UPS</td>\n      <td>Rome</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>231932</td>\n      <td>SKU_1501</td>\n      <td>Headphones</td>\n      <td>49</td>\n      <td>2020-01-01 02:00</td>\n      <td>29.11</td>\n      <td>50472.0</td>\n      <td>Germany</td>\n      <td>0.350000</td>\n      <td>Bank Transfer</td>\n      <td>23.03</td>\n      <td>Electronics</td>\n      <td>Online</td>\n      <td>Returned</td>\n      <td>UPS</td>\n      <td>Berlin</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>465838</td>\n      <td>SKU_1760</td>\n      <td>Desk Lamp</td>\n      <td>14</td>\n      <td>2020-01-01 03:00</td>\n      <td>76.68</td>\n      <td>96586.0</td>\n      <td>Netherlands</td>\n      <td>0.140000</td>\n      <td>paypall</td>\n      <td>11.08</td>\n      <td>Accessories</td>\n      <td>Online</td>\n      <td>Not Returned</td>\n      <td>Royal Mail</td>\n      <td>Rome</td>\n      <td>Low</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>359178</td>\n      <td>SKU_1386</td>\n      <td>USB Cable</td>\n      <td>-30</td>\n      <td>2020-01-01 04:00</td>\n      <td>-68.11</td>\n      <td>NaN</td>\n      <td>United Kingdom</td>\n      <td>1.501433</td>\n      <td>Bank Transfer</td>\n      <td>NaN</td>\n      <td>Electronics</td>\n      <td>In-store</td>\n      <td>Not Returned</td>\n      <td>FedEx</td>\n      <td>NaN</td>\n      <td>Medium</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#data inspection process - check for missing values\nmissing_values = df.isna().sum()\nmissing_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.524809Z","iopub.execute_input":"2025-11-24T18:39:21.525079Z","iopub.status.idle":"2025-11-24T18:39:21.562633Z","shell.execute_reply.started":"2025-11-24T18:39:21.525058Z","shell.execute_reply":"2025-11-24T18:39:21.561730Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"InvoiceNo               0\nStockCode               0\nDescription             0\nQuantity                0\nInvoiceDate             0\nUnitPrice               0\nCustomerID           4978\nCountry                 0\nDiscount                0\nPaymentMethod           0\nShippingCost         2489\nCategory                0\nSalesChannel            0\nReturnStatus            0\nShipmentProvider        0\nWarehouseLocation    3485\nOrderPriority           0\ndtype: int64"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Data Processing Pipeline\n\n**Data cleaning: type normalization**\n - Convert InvoiceDate to datetime (coerce errors)\n - Ensure numeric columns (Quantity, UnitPrice, Discount, ShippingCost)\n - Provide a short printed summary of conversions and any missing values\n\n**Data cleaning: fill categorical and standardize text**\n - Fill missing CustomerID, Category, SalesChannel, etc.\n - Strip strings and standardize casing if needed\n - Explain why these fills are acceptable (e.g., 'Unknown' placeholder)\n\n**Data cleaning: normalize discount percentages and detect returns**\n - Convert 1-100 values to 0-1 where relevant\n - Create IsReturn flag from quantity and return text heuristics\n - Add comments explaining heuristics used\n\n**Save cleaned DataFrame to a new variable**\n - For faster iteration you may want to save a cleaned copy: df.to_parquet(...) or to CSV\n - Note: make sure sanitized output does not store sensitive data","metadata":{}},{"cell_type":"code","source":"# Load dataset: Function to load the CSV into a DataFrame and show basic info:\n\ndef load_sales_data(csv_path: str) -> pd.DataFrame:\n    \"\"\"Load CSV and ensure proper types.\"\"\"\n    df = pd.read_csv(csv_path, low_memory=False)\n    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n    \n    # Ensure numeric columns\n    for col in ['Quantity', 'UnitPrice', 'Discount', 'ShippingCost']:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)\n    if 'Quantity' in df.columns:\n        df['Quantity'] = df['Quantity'].astype(int)\n    \n    return df\n\n\n# Clean dataset: Function to clean and preprocess the dataset\n\ndef clean_sales_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Fill missing values, standardize text, normalize discounts, compute revenue and returns.\"\"\"\n    # Fill missing categorical info\n    cat_fill = {\n        'CustomerID': 'Unknown',\n        'WarehouseLocation': 'Unknown',\n        'Description': '',\n        'Category': 'Unknown',\n        'SalesChannel': 'Unknown',\n        'ReturnStatus': 'No Return'\n    }\n    for col, val in cat_fill.items():\n        if col in df.columns:\n            df[col] = df[col].fillna(val)\n    \n    # Standardize small text fields\n    for col in ['PaymentMethod','Country','ShipmentProvider','OrderPriority']:\n        if col in df.columns:\n            df[col] = df[col].fillna('Unknown').astype(str).str.strip()\n    \n    # Normalize discounts (percentages to 0-1 range)\n    if 'Discount' in df.columns:\n        disc = df['Discount'].copy()\n        mask_pct = (disc > 1) & (disc <= 100)\n        disc.loc[mask_pct] = disc.loc[mask_pct] / 100.0\n        df['Discount_norm'] = disc.clip(lower=0.0).fillna(0.0)\n    \n    # Determine returns\n    return_indicators = ['return', 'returned', 'refunded', 'rma']\n    df['IsReturnFlag_text'] = df['ReturnStatus'].astype(str).str.lower().fillna('')\n    df['IsReturn_by_text'] = df['IsReturnFlag_text'].apply(lambda s: any(k in s for k in return_indicators))\n    df['IsReturn_by_qty'] = df['Quantity'] < 0\n    df['IsReturn'] = df['IsReturn_by_text'] | df['IsReturn_by_qty']\n    \n    # Compute line revenue once\n    df['LineRevenue'] = df['Quantity'] * df['UnitPrice'] * (1.0 - df.get('Discount_norm', 0.0))\n    df['NetRevenue'] = df['LineRevenue']\n    \n    # Drop temporary helper columns\n    df.drop(columns=['IsReturnFlag_text','IsReturn_by_text','IsReturn_by_qty'], inplace=True, errors='ignore')\n    \n    return df\n\n\n# Processing pipeline: Process the first few functions and turn them into a pipeline that can save the cleaned file\n\ndef process_sales_pipeline(csv_path: str) -> pd.DataFrame:\n    \"\"\"Full pipeline: load + clean sales data.\"\"\"\n    df = load_sales_data(csv_path)\n    df_clean = clean_sales_data(df)\n    return df_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.563754Z","iopub.execute_input":"2025-11-24T18:39:21.564009Z","iopub.status.idle":"2025-11-24T18:39:21.577353Z","shell.execute_reply.started":"2025-11-24T18:39:21.563989Z","shell.execute_reply":"2025-11-24T18:39:21.576523Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Daily Report Function\n\n**Build daily report**\n - Function: collects cleaned dataframe, requested date, looks back 28 days from there to understand trends and anomalies\n - Computes metrics: total_revenue, num_orders, aov, top_products, revenue_by_category, revenue_by_channel, top_countries, return_rate, anomalies, trend\n - Ensure outputs are JSON-safe: keys are strings, values are native Python types\n - Add a short doctest or example run to show the expected output shape\n\n**Format helpers**\n - fmt_money(value) -> returns formatted string and numeric round for monetary metrics","metadata":{}},{"cell_type":"code","source":"#Money format helper\ndef fmt_money(x):\n    try:\n        return f\"${x:,.2f}\"\n    except:\n        return x\n\n\n#first report calculator function\ndef build_daily_report(df_all, target_date_str, lookback_days_for_trend=28):\n    \"\"\"\n    Returns:\n      day_df: DataFrame with all transactions for target_date (date portion match)\n      metrics: dict with totals, AOV, top_products, revenue_by_category/channel, return_rate, anomalies, short trend context\n    \"\"\"\n    # parse target date as date-only\n    target_dt = pd.to_datetime(target_date_str).normalize()\n    # Ensure InvoiceDate is timezone-naive for date comparisions\n    try:\n        tzinfo = getattr(df_all['InvoiceDate'].dt, 'tz', None)\n    except Exception:\n        tzinfo = None\n    \n    if tzinfo is not None:\n        df_all['InvoiceDate_naive'] = df_all['InvoiceDate'].dt.tz_convert(None)\n    else:\n        df_all['InvoiceDate_naive'] = df_all['InvoiceDate']\n    df_all['InvoiceDate_date'] = pd.to_datetime(df_all['InvoiceDate_naive']).dt.normalize()\n    \n    # Subset for the exact date\n    day_df = df_all[df_all['InvoiceDate_date'] == target_dt].copy().reset_index(drop=True)\n    \n    # Basic metrics\n    total_revenue = float(day_df['NetRevenue'].sum()) if len(day_df) else 0.0\n    # unique invoices for that day\n    num_orders = int(day_df['InvoiceNo'].nunique()) if len(day_df) else 0\n    num_lines = int(len(day_df))\n    avg_order_value = (total_revenue / num_orders) if num_orders else 0.0\n    avg_line_value = (day_df['NetRevenue'].mean() if num_lines else 0.0)\n    \n    # top products by revenue\n    top_products = day_df.groupby(['StockCode','Description'])['NetRevenue'].sum().sort_values(ascending=False).head(10)\n    top_products_list = [{'StockCode': sc, 'Description': desc, 'revenue': float(v)} \n                         for (sc,desc), v in top_products.items()]\n    \n    # revenue breakdowns\n    revenue_by_category = day_df.groupby('Category')['NetRevenue'].sum().sort_values(ascending=False).to_dict()\n    revenue_by_channel = day_df.groupby('SalesChannel')['NetRevenue'].sum().sort_values(ascending=False).to_dict()\n    \n    # top countries\n    top_countries = (\n        day_df.groupby('Country')['NetRevenue']\n        .sum()\n        .sort_values(ascending=False)\n        .head(5)\n    )\n    top_countries_list = top_countries.reset_index().to_dict(orient='records')\n    \n    # return rate (by line)\n    if len(day_df):\n        returns_count = int(day_df['IsReturn'].sum())\n        return_rate = returns_count / len(day_df)\n    else:\n        returns_count = 0\n        return_rate = 0.0\n    \n    # anomaly detection (per-line) -- simple z-score threshold\n    anomalies = []\n    if len(day_df) >= 5:\n        mean_line = day_df['NetRevenue'].mean()\n        std_line = day_df['NetRevenue'].std(ddof=0)\n        if pd.isna(std_line) or std_line == 0:\n            std_line = 0.0\n        thresh_upper = mean_line + 3 * std_line\n        thresh_lower = mean_line - 3 * std_line\n        anom_df = day_df[(day_df['NetRevenue'] > thresh_upper) | (day_df['NetRevenue'] < thresh_lower)]\n        anomalies = anom_df[['InvoiceNo','StockCode','Description','Quantity','UnitPrice','NetRevenue']].to_dict('records')\n    \n    # short trend context: compare revenue to previous week/day and percentage change\n    # compute summed revenue per day for lookback window\n    start_trend = target_dt - pd.Timedelta(days=lookback_days_for_trend)\n    mask_trend = (df_all['InvoiceDate_date'] >= start_trend) & (df_all['InvoiceDate_date'] <= target_dt)\n    series_daily = df_all.loc[mask_trend].groupby('InvoiceDate_date')['NetRevenue'].sum().sort_index()\n    trend = {}\n    if not series_daily.empty:\n        # previous day\n        prev_day = target_dt - pd.Timedelta(days=1)\n        prev_revenue = float(series_daily.get(prev_day, 0.0))\n        prev_change_pct = ((total_revenue - prev_revenue) / prev_revenue * 100.0) if prev_revenue else None\n        # 7-day avg before target day (exclude target day)\n        week_start = target_dt - pd.Timedelta(days=7)\n        week_mask = (series_daily.index >= week_start) & (series_daily.index < target_dt)\n        week_avg = float(series_daily.loc[week_mask].mean()) if series_daily.loc[week_mask].size else None\n        week_change_pct = ((total_revenue - week_avg) / week_avg * 100.0) if week_avg not in (None, 0) else None\n        trend = {\n            'prev_day_revenue': prev_revenue,\n            'prev_day_change_pct': prev_change_pct,\n            'week_avg_before': week_avg,\n            'week_change_pct': week_change_pct,\n            'series_daily': series_daily.to_dict()\n        }\n    else:\n        trend = {'series_daily': {}}\n\n\n    # ensure consistent key names and format money for lists\n    for item in top_products_list:\n        # accept 'revenue' or 'NetRevenue' and write back to 'revenue'\n        raw = item.get('NetRevenue', 0.0)\n        item['revenue'] = fmt_money(raw)\n    \n    for item in top_countries_list:\n        raw = item.get('NetRevenue', 0.0)\n        item['revenue'] = fmt_money(raw)\n\n    # Top products by revenue -> dict {Description: revenue}\n    top_products_series = day_df.groupby('Description')['NetRevenue'].sum().sort_values(ascending=False).head(10)\n    top_products = {desc: round(float(v), 2) for desc, v in top_products_series.items()}\n\n    # Top countries by revenue -> dict {Country: revenue}\n    top_countries_series = day_df.groupby('Country')['NetRevenue'].sum().sort_values(ascending=False).head(5)\n    top_countries = {country: round(float(v), 2) for country, v in top_countries_series.items()}\n\n    metrics = {\n        'date': target_dt.strftime('%Y-%m-%d'),\n        'total_revenue': fmt_money(total_revenue),\n        'num_orders': num_orders,\n        'num_lines': num_lines,\n        'avg_order_value': fmt_money(avg_order_value),\n        'avg_line_value': fmt_money(avg_line_value),\n        'top_products': top_products_list,\n        'revenue_by_category': {k: float(v) for k,v in revenue_by_category.items()},\n        'revenue_by_channel': {k: float(v) for k,v in revenue_by_channel.items()},\n        'top_countries': top_countries_list,\n        'returns_count': returns_count,\n        'return_rate': return_rate,\n        'anomalies': anomalies,\n        'trend': trend\n    }\n    \n    return day_df, metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.578355Z","iopub.execute_input":"2025-11-24T18:39:21.578577Z","iopub.status.idle":"2025-11-24T18:39:21.952898Z","shell.execute_reply.started":"2025-11-24T18:39:21.578559Z","shell.execute_reply":"2025-11-24T18:39:21.951892Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Pipeline for the report builder function\n\ndef run_daily_report_for_date(date_str: str, df=csv_path):\n    \"\"\"\n    Run daily report for a given date on a pre-cleaned DataFrame.\n    \n    Parameters:\n        df : pd.DataFrame\n            Cleaned sales DataFrame (output of process_sales_pipeline)\n        date_str : str\n            Date string in 'YYYY-MM-DD' format to run the report\n    \n    Returns:\n        day_transactions_df : pd.DataFrame\n            All transactions for the selected date\n        metrics : dict\n            Calculated daily metrics in flattened key-value format\n    \"\"\"\n    # Load and clean the dataset\n    # df = process_sales_pipeline(csv_path)\n    \n    # Ensure date column exists\n    if 'InvoiceDate_date' not in df.columns:\n        df['InvoiceDate_date'] = pd.to_datetime(df['InvoiceDate']).dt.normalize()\n    \n    # Normalize selected date\n    target_dt = pd.to_datetime(date_str).normalize()\n    \n    # Check if date exists in dataset\n    if target_dt not in df['InvoiceDate_date'].values:\n        print(f\"Warning: {date_str} not found in dataset. Using closest available date instead.\")\n        target_dt = df['InvoiceDate_date'].min()\n    \n    target_date_str = target_dt.strftime('%Y-%m-%d')\n\n    # Run daily report\n    day_transactions_df, metrics = build_daily_report(df, target_date_str)\n    \n    # Display summary\n    print(\"Metrics summary:\")\n    for k, v in metrics.items():\n        if k in ('top_products', 'top_countries', 'anomalies', 'trend'):\n            print(f\"{k}: (see structure) - {type(v)}\")\n        else:\n            print(f\"{k}: {v}\")\n    \n    return day_transactions_df, metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.953730Z","iopub.execute_input":"2025-11-24T18:39:21.954056Z","iopub.status.idle":"2025-11-24T18:39:21.979083Z","shell.execute_reply.started":"2025-11-24T18:39:21.954022Z","shell.execute_reply":"2025-11-24T18:39:21.978164Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"#### Test run the above function with these codes\n**Test: Run report for a specific date**\n\nmetrics = run_daily_report_for_date(\"2020-01-11\")\nmetrics","metadata":{}},{"cell_type":"markdown","source":"## Google ADK Response Helper","metadata":{}},{"cell_type":"code","source":"# Define helper functions that will be reused throughout the notebook\nasync def run_session(\n    runner_instance: Runner,\n    user_queries: list[str] | str = None,\n    session_name: str = \"default\",\n):\n    print(f\"\\n ### Session: {session_name}\")\n\n    # Get app name from the Runner\n    app_name = runner_instance.app_name\n\n    # Attempt to create a new session or retrieve an existing one\n    try:\n        session = await session_service.create_session(\n            app_name=app_name, user_id=USER_ID, session_id=session_name\n        )\n    except:\n        session = await session_service.get_session(\n            app_name=app_name, user_id=USER_ID, session_id=session_name\n        )\n\n    # Process queries if provided\n    if user_queries:\n        # Convert single query to list for uniform processing\n        if type(user_queries) == str:\n            user_queries = [user_queries]\n\n        # Process each query in the list sequentially\n        for query in user_queries:\n            print(f\"\\nUser > {query}\")\n\n            # Convert the query string to the ADK Content format\n            query = types.Content(role=\"user\", parts=[types.Part(text=query)])\n\n            # Stream the agent's response asynchronously\n            async for event in runner_instance.run_async(\n                user_id=USER_ID, session_id=session.id, new_message=query\n            ):\n                # Check if the event contains valid content\n                if event.content and event.content.parts:\n                    # Filter out empty or \"None\" responses before printing\n                    if (\n                        event.content.parts[0].text != \"None\"\n                        and event.content.parts[0].text\n                    ):\n                        print(f\"{MODEL_NAME} > \", event.content.parts[0].text)\n    else:\n        print(\"No queries!\")\n\n\nretry_config = types.HttpRetryOptions(\n    attempts=5,  # Maximum retry attempts\n    exp_base=7,  # Delay multiplier\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n)\n\nprint(\"âœ… Retry has been successfully configured.\")\nprint(\"âœ… Helper functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:21.980816Z","iopub.execute_input":"2025-11-24T18:39:21.981168Z","iopub.status.idle":"2025-11-24T18:39:22.008886Z","shell.execute_reply.started":"2025-11-24T18:39:21.981142Z","shell.execute_reply":"2025-11-24T18:39:22.007870Z"}},"outputs":[{"name":"stdout","text":"âœ… Retry has been successfully configured.\nâœ… Helper functions defined.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Tool Sanitizers and Wrappers\n\n**Tool sanitizers and wrappers**\n - sanitize_for_json(metrics) that converts Timestamp keys to strings and numpy scalars to Python types\n - helper to convert DataFrame slices into list-of-dicts for safe return by tools","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, date, timedelta\n\ndef sanitize_for_json(obj):\n    \"\"\"\n    Recursively convert an object into JSON-serializable Python primitives:\n      - pandas.Timestamp -> 'YYYY-MM-DD' string or ISO\n      - pandas.Timedelta -> string\n      - numpy scalar -> native python type\n      - numpy arrays -> lists\n      - decimal.Decimal -> float\n      - pandas/np NaN/NaT -> None\n    Also converts dict keys that are datetime-like to strings.\n    \"\"\"\n    # primitives\n    if obj is None:\n        return None\n    if isinstance(obj, (str, bool, int, float)):\n        # cover normal Python primitives\n        if isinstance(obj, float) and (np.isnan(obj) or np.isinf(obj)):\n            return None\n        return obj\n\n    # pandas / numpy scalars\n    if isinstance(obj, (np.generic,)):\n        return obj.item()\n\n    if isinstance(obj, (pd.Timestamp, datetime, date)):\n        # choose date-only format for Timestamp to match your pipeline\n        try:\n            return pd.to_datetime(obj).strftime('%Y-%m-%d')\n        except Exception:\n            return str(obj)\n\n    if isinstance(obj, pd.Timedelta):\n        return str(obj)\n\n    if isinstance(obj, decimal.Decimal):\n        return float(obj)\n\n    # pandas NA\n    if obj is pd.NaT or (isinstance(obj, float) and np.isnan(obj)):\n        return None\n\n    # dict: sanitize keys and values\n    if isinstance(obj, dict):\n        new = {}\n        for k, v in obj.items():\n            # convert key to str if not simple type\n            if isinstance(k, (pd.Timestamp, datetime, date)):\n                new_key = pd.to_datetime(k).strftime('%Y-%m-%d')\n            elif isinstance(k, (np.generic,)):\n                new_key = str(k.item())\n            elif not isinstance(k, (str, int, float, bool, type(None))):\n                new_key = str(k)\n            else:\n                new_key = k\n            new[new_key] = sanitize_for_json(v)\n        return new\n\n    # list/tuple/set -> list\n    if isinstance(obj, (list, tuple, set)):\n        return [sanitize_for_json(v) for v in obj]\n\n    # pandas Series -> dict with string keys\n    if isinstance(obj, pd.Series):\n        return sanitize_for_json(obj.to_dict())\n\n    # pandas DataFrame -> list of records (safe)\n    if isinstance(obj, pd.DataFrame):\n        # convert DataFrame to list of dicts with sanitized values\n        records = obj.to_dict(orient='records')\n        return sanitize_for_json(records)\n\n    # numpy arrays\n    if isinstance(obj, np.ndarray):\n        return [sanitize_for_json(v) for v in obj.tolist()]\n\n    # fallback: try to cast to primitive\n    try:\n        return float(obj)\n    except Exception:\n        try:\n            return str(obj)\n        except Exception:\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:22.012487Z","iopub.execute_input":"2025-11-24T18:39:22.012856Z","iopub.status.idle":"2025-11-24T18:39:22.035810Z","shell.execute_reply.started":"2025-11-24T18:39:22.012831Z","shell.execute_reply":"2025-11-24T18:39:22.034662Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# preload df_clean once\ndf_clean = process_sales_pipeline(csv_path)  # run this earlier\n\n# ensure you have a df-based runner\ndef run_daily_report_for_date_df(df: pd.DataFrame, date_str: str):\n    if 'InvoiceDate_date' not in df.columns:\n        df['InvoiceDate_date'] = pd.to_datetime(df['InvoiceDate']).dt.normalize()\n\n    target_dt = pd.to_datetime(date_str).normalize()\n    if target_dt not in df['InvoiceDate_date'].values:\n        target_dt = df['InvoiceDate_date'].min()\n    target_date_str = target_dt.strftime('%Y-%m-%d')\n\n    day_transactions_df, metrics = build_daily_report(df, target_date_str)\n    return day_transactions_df, metrics\n\n# ADK-friendly tool function - accepts only a date string and returns sanitized JSON-friendly dict\ndef daily_report_tool_for_agent(date_str: str):\n    try:\n        target_date = pd.to_datetime(date_str).strftime('%Y-%m-%d')\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": f\"Invalid date format: {date_str}\"}\n\n    try:\n        _, metrics = run_daily_report_for_date_df(df_clean, target_date)\n        metrics_safe = sanitize_for_json(metrics)\n        return {\"status\": \"success\", \"metrics\": metrics_safe}\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:22.036941Z","iopub.execute_input":"2025-11-24T18:39:22.037219Z","iopub.status.idle":"2025-11-24T18:39:22.423287Z","shell.execute_reply.started":"2025-11-24T18:39:22.037197Z","shell.execute_reply":"2025-11-24T18:39:22.422118Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Agent Definitions\n\n**daily_report_agent definition**\n - Calls daily_report_tool_for_agent\n - Instruction: parse date from prompt, call tool, return metrics\n\n**summary_agent definition**\n - Calls generate_executive_summary_for_agent\n - Instruction: read sanitized metrics and return one-paragraph executive summary\n\n**followup_agent definition**\n - Session-based conversation agent for follow-ups\n - Expects SESSION_JSON + USER_MESSAGE as input\n - Returns short strategic reply or \"CONFIRMED_RECOMPUTE_REQUESTED\" token for recompute flows\n\n**orchestrator_agent definition**\n - Orchestrates daily_report_agent and summary tool\n - Instruction: parse date, call daily_report_agent, then summary tool, and merge outputs into final report text","metadata":{}},{"cell_type":"code","source":"# Daily report processing agent is defined here\n\ndaily_report_agent = LlmAgent(\n    name=\"daily_report_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    instruction=\"\"\"\nYou are a daily report assistant.\n1) Extract the date from the user's prompt.\n2) Convert it to 'YYYY-MM-DD'.\n3) Call daily_report_tool_for_agent(date_str).\n4) Return the tool output exactly if status == 'success', otherwise explain the error.\n\"\"\",\n    tools=[daily_report_tool_for_agent],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:22.424329Z","iopub.execute_input":"2025-11-24T18:39:22.424692Z","iopub.status.idle":"2025-11-24T18:39:22.430994Z","shell.execute_reply.started":"2025-11-24T18:39:22.424660Z","shell.execute_reply":"2025-11-24T18:39:22.429688Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"##### Test the above agent with these codes:\n\nrunner = InMemoryRunner(agent=daily_report_agent)\n\nprint(\"âœ… Runner created.\")\n\nresponse = await runner.run_debug(\"What's my report for today, 2020-01-11\")","metadata":{}},{"cell_type":"code","source":"##### Test the above agent with these codes:\n\nrunner = InMemoryRunner(agent=daily_report_agent)\n\nprint(\"âœ… Runner created.\")\n\nresponse = await runner.run_debug(\"What's my report for today, 2020-01-11\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:22.432108Z","iopub.execute_input":"2025-11-24T18:39:22.432444Z","iopub.status.idle":"2025-11-24T18:39:25.571056Z","shell.execute_reply.started":"2025-11-24T18:39:22.432411Z","shell.execute_reply":"2025-11-24T18:39:25.569758Z"}},"outputs":[{"name":"stdout","text":"âœ… Runner created.\n\n ### Created new session: debug_session_id\n\nUser > What's my report for today, 2020-01-11\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"daily_report_agent > Here is your report for 2020-01-11:\n\n**Overall Metrics:**\n*   Total Revenue: $25,497.17\n*   Number of Orders: 24\n*   Number of Lines: 24\n*   Average Order Value: $1,062.38\n*   Average Line Value: $1,062.38\n*   Return Rate: 1%\n\n**Revenue by Category:**\n*   Accessories: $5,121.69\n*   Apparel: $6,513.51\n*   Electronics: $5,447.78\n*   Furniture: $7,302.08\n*   Stationery: $1,112.11\n\n**Revenue by Channel:**\n*   In-store: $7,314.65\n*   Online: $18,182.51\n\n**Top Countries by Revenue:**\n1.  United States: $4,560.67\n2.  Spain: $4,121.89\n3.  France: $3,325.39\n4.  Portugal: $2,557.36\n5.  Netherlands: $2,219.13\n\n**Top Products:**\n*   USB Cable (SKU_1961): $0.00\n*   Wireless Mouse (SKU_1477): $0.00\n*   Office Chair (SKU_1982): $0.00\n*   Office Chair (SKU_1964): $0.00\n*   Office Chair (SKU_1589): $0.00\n*   Office Chair (SKU_1471): $0.00\n*   Wireless Mouse (SKU_1020): $0.00\n*   T-shirt (SKU_1297): $0.00\n*   Wall Clock (SKU_1936): $0.00\n*   USB Cable (SKU_1348): $0.00\n\n**Trend:**\n*   There was a 15.91% increase in revenue compared to the previous day.\n*   The total revenue of $25,497.17 represents a 25.88% increase compared to the average weekly revenue before this date.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Executive Summary processing agent is defined here\n\nsummary_agent = LlmAgent(\n    name=\"summary_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    instruction=\"\"\"\nYou are an executive summarizer. Your job is to call the prior agent to get the sanitized metrics, then turn those metrics into a single, readable executive paragraph the user can act on.\n\nAgent behavior and process\n1) Obtain metrics\n   - Call the daily_report_agent tool and get back the sanitized metrics object.\n   - Treat that metrics object as authoritative. Do not invent values.\n\n2) What to read from metrics (use when present)\n   - date / report_date\n   - total_revenue\n   - num_orders\n   - avg_order_value or avg_order_value\n   - revenue_by_category (dict)\n   - revenue_by_channel (dict)\n   - top_products (dict) pick top 3 by value\n   - top_countries (dict) pick top 3 by value\n   - anomalies (dict) use count and up to 2 example keys\n   - trend (dict) use _prev_day_change_pct and _week_change_pct when present\n\n3) Tone and style\n   - Write in plain, engaging English aimed at a manager.\n   - Use varied sentence openers and short sentences for impact.\n   - Avoid dry lists. Merge related facts into compact sentences.\n   - If the report contains surprising or missing numbers (for example many top product revenues are zero) call that out briefly as a data quality note. Do not speculate on causes.\n\n4) Formatting rules\n   - Produce exactly one paragraph of 4 to 6 sentences.\n   - Round monetary values to two decimals and format with commas, for example $12,345.67.\n   - Format percentages to two decimals and append %, for example 12.34%.\n   - When stating a percentage change, also include the absolute reference if available, for example \"up 15.9% (from $20,000 to $23,180)\". If the absolute figure is not available, give the percent alone.\n\n5) Sentence structure (recommended, not template locked)\n   - Sentence 1: Headline. Date, total revenue, orders. Short and punchy.\n     Example: \"Executive summary for 2020-01-11: total revenue was $25,497.17 from 24 orders.\"\n   - Sentence 2: Channel and AOV highlight. Mention top channel and AOV.\n     Example: \"Average order value was $1,062.38, with Online leading at $18,182.51.\"\n   - Sentence 3: Category highlight. State top category and its revenue, with one short qualifier if relevant.\n   - Sentence 4: Top products. List the top 3 product names with revenue. If those revenues are zero or missing, list the product names and add \"revenues appear missing\" as a data note.\n   - Sentence 5: Geography and anomalies. Top 3 countries, then anomaly count and up to two example anomaly keys or \"no anomalies detected.\"\n   - Sentence 6: Trend insight. State day-over-day and week-over-week percent changes and what they imply for the business direction. If trend numbers are missing, say \"trend data not available.\"\n\n6) Output constraints\n   - Return only the single paragraph string as markdown text. No bullet lists. No extra metadata. No apologies, no filler sentences.\n   - If the tool call fails or returns an error, return a single sentence error message that includes the tool's message and one concrete corrective step, for example: \"Tool error: <message>. Suggestion: re-run with sanitized metrics that include total_revenue and top_products.\"\n\n7) Data quality guardrails\n   - If more than half of the top products show $0.00, append a short note at the end: \"Note: many top product revenues are zero; verify source data.\"\n   - Never invent causal explanations. If the user asks why, respond that the metrics do not contain causal data and offer next steps to investigate.\n\nImplementation detail\n- First call the daily_report_agent tool to fetch metrics.\n- Then compose the paragraph locally following the rules above.\n- Return the paragraph exactly as the agent output.\n\nExample final paragraph style:\n\"Executive summary for 2020-01-11: total revenue was $25,497.17 from 24 orders. Average order value was $1,062.38, with Online driving the largest share at $18,182.51. Furniture was the top category at $7,302.08. Top products were Office Chair (SKU_1982) $X, Wireless Mouse (SKU_1477) $Y, USB Cable (SKU_1961) $Z. Top countries were United States ($4,560.67), Spain ($4,121.89), France ($3,325.40); no anomalies detected. Sales are up 15.91% day-over-day and 25.88% versus the weekly average.\"\n\"\"\",\n    tools=[\n        AgentTool(agent=daily_report_agent),\n    ],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:25.572682Z","iopub.execute_input":"2025-11-24T18:39:25.572998Z","iopub.status.idle":"2025-11-24T18:39:25.580061Z","shell.execute_reply.started":"2025-11-24T18:39:25.572968Z","shell.execute_reply":"2025-11-24T18:39:25.578821Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"##### Test the above agent with these codes:\n\nsummary_runner = InMemoryRunner(agent=summary_agent)\n\nprint(\"âœ… Runner created.\")\n\nsummary_response = await summary_runner.run_debug(\"What's my report for today, 2020-01-11\")","metadata":{}},{"cell_type":"code","source":"##### Test the above agent with these codes:\n\nsummary_runner = InMemoryRunner(agent=summary_agent)\n\nprint(\"âœ… Runner created.\")\n\nsummary_response = await summary_runner.run_debug(\"What's my report for today, 2020-01-11\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:25.581000Z","iopub.execute_input":"2025-11-24T18:39:25.581317Z","iopub.status.idle":"2025-11-24T18:39:30.213368Z","shell.execute_reply.started":"2025-11-24T18:39:25.581292Z","shell.execute_reply":"2025-11-24T18:39:30.212464Z"}},"outputs":[{"name":"stdout","text":"âœ… Runner created.\n\n ### Created new session: debug_session_id\n\nUser > What's my report for today, 2020-01-11\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\nWARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"summary_agent > Executive summary for 2020-01-11: total revenue was $25,497.17 from 24 orders. Average order value was $1,062.38, with Online driving the largest share at $18,182.51. Furniture was the top category at $7,302.08. Top products include USB Cable (SKU_1961), Wireless Mouse (SKU_1477), and Office Chair (SKU_1982). Top countries were United States ($4,560.67), Spain ($4,121.89), and France ($3,325.40); no anomalies detected. Sales are up 15.91% day-over-day and 25.88% versus the weekly average.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Follow-up response agent is defined here\n\nfollowup_agent = LlmAgent(\n    name=\"followup_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    instruction=\"\"\"\nYou are the session-based conversational follow-up assistant for a single report session. You are to take the persona of a strategy-driven analyst.\n\nYour responsibilities\n1) Pick from the session:\n   - Pick your information from the memory session provided by the system and answer the user's questions as directly as you can.\n\n2) Use only session state for answers:\n   - You should cite values from metrics, and summary from the output of the other agents.\n   - Never call external data or recompute unless user explicitly requests and confirms a recompute.\n   - Do not invent, infer, or extrapolate beyond the session data. If information is missing, answer by saying the exact field is \"data not available\".\n\n3) Follow-up behavior and allowed actions:\n   - Answer concise, manager-friendly follow-up questions (1â€“5 sentences).\n   - Make your message strategy-driven (strategic) in nature\n   - If the user asks for deeper analysis that requires re-running pipelines (for example \"recompute with filter country=Spain\" or \"show full order list for SKU X\"), ask for explicit confirmation before performing the recompute: respond with a question like \"To run that analysis I will re-run the pipeline for 2020-01-11 and consume one follow-up. Reply 'yes' to confirm.\" Do not run anything until you receive that exact confirmation in the next user message.\n   - If the user confirms (message equals \"yes\" or \"confirm recompute\"), respond with exactly: \"CONFIRMED_RECOMPUTE_REQUESTED\" so the caller wrapper can perform the recompute and update the session. Do not include other text.\n   - If the user asks for data already present in transactions_preview, return up to 3 example rows. Format each example as a single short sentence: \"Example order: InvoiceNo=<>, Description=<>, NetRevenue=$<>, Country=<>.\"\n\n4) Tone and content rules:\n   - Use plain English aimed at managers. Keep answers short and useful.\n   - Your answers should be written in markdown format only.\n   - When you reference numbers, format money with commas and two decimals (e.g., $12,345.67) and percentages to two decimals (e.g., 12.34%).\n   - If more than half of top_products show $0.00, include a single-sentence data-quality note: \"Note: many top product revenues are zero; verify source data.\"\n   - Never provide causal claims. If asked \"why\" beyond the data, reply: \"I can't determine causes from the metrics. I can run deeper analysis if you confirm.\"\n\n5) Examples of allowed replies\n   - Short data reply: â€œOffice Chair remained the strongest contributor with revenue of $7,302.08. This suggests demand is steady for higher-value practical items. A practical next step is to review margin performance on this product line and confirm whether inventory levels can support similar demand over the next week. If needed, I can provide example transactions from today to help you check pricing consistency.â€\n   - Recompute ask: \"To run a deeper SKU breakdown I need to re-run the pipeline and it will consume one follow-up. Reply 'yes' to confirm.\"\n\n6) Safety and fidelity\n   - If the user asks exceeds 3 allowed follow-ups, refuse with the single-line message: \"Follow-up limit reached. No more follow-ups allowed for this session.\"\n\nRemember: The agent must never modify session storage directly. It should return plain text only and emit \"CONFIRMED_RECOMPUTE_REQUESTED\" when the user confirms recompute.\n\"\"\",\n    tools=[],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:30.214784Z","iopub.execute_input":"2025-11-24T18:39:30.215325Z","iopub.status.idle":"2025-11-24T18:39:30.221680Z","shell.execute_reply.started":"2025-11-24T18:39:30.215291Z","shell.execute_reply":"2025-11-24T18:39:30.220440Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#Orchestrator agent runs all the pipeline together\n\norchestrator_agent = LlmAgent(\n    name=\"orchestrator_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    instruction=\"\"\"\nYou are the Orchestrator Agent. Your job is to run the daily_report_agent to fetch metrics, call the summary tool to create an executive summary, and present a single final report to the manager.\n\nFollow these steps exactly:\n\n1) Parse the user's prompt for a date:\n   - Accept ISO \"YYYY-MM-DD\", common formats like \"Feb 18 2024\", \"18/02/2024\", or phrases like \"today\" (resolve \"today\" to current date in YYYY-MM-DD).\n   - Convert to normalized string 'YYYY-MM-DD'. If parsing fails, return a single-line error: \"Error: could not parse date. Please provide a date like YYYY-MM-DD.\"\n\n2) Call the daily report tool:\n   - Call the tool provided that wraps the daily_report_agent with the date string as its single argument.\n   - Expect a tool response object with a `status` field. If `status != \"success\"`, return exactly:\n     \"Tool error: <message>. Suggestion: check dataset availability and date correctness.\"\n   - On success, extract the `metrics` dict from the tool response.\n\n3) Call the summary tool:\n   - Convert `metrics` to a JSON string: `metrics_json = json.dumps(metrics)`.\n   - Call `generate_executive_summary_for_agent(metrics_json)`.\n   - If tool returns `status != \"success\"`, return exactly:\n     \"Summary tool error: <message>. Suggestion: ensure metrics JSON is sanitized.\"\n   - On success, extract `summary` string.\n\n4) Call the follow-up agent tool:\n\tâ€¢\tCall the tool that wraps the followup_agent, passing the session memory or output from the other 2 agents and the userâ€™s follow-up message.\n\tâ€¢\tExpect a short paragraph in markdown format\n\tâ€¢\tExtract the reply string from the agent and surface it directly to the user.\n\n5) Merge and present daily report:\n   - Compose the final output exactly as Markdown plain text with two parts:\n\n     A) The executive summary paragraph as-is from the summary_agent (no changes).\n\n     B) A compact \"Key metrics\" block below the paragraph (no tables, use short labeled lines). \n        Include the following fields if present:\n        - date\n        - total_revenue\n        - num_orders\n        - avg_order_value\n        - top_category (name + value)\n        - top_channel (name + value)\n        - top_countries (top 3 with values)\n        - anomalies_count (number of anomalies) and up to two anomaly names if available\n        - trend_day_over_day (formatted percent), if available\n        - trend_week_over_week (formatted percent), if available\n\n        Format money with commas and two decimals (e.g., $12,345.67).\n        Format percentages with two decimals (e.g., 15.91%).\n\n   - Example final format:\n     <summary paragraph>\n\n     Key metrics:\n     - Date: 2020-01-11\n     - Total revenue: $25,497.17\n     - Orders: 24\n     - AOV: $1,062.38\n     - Top category: Furniture ($7,302.08)\n     - Top channel: Online ($18,182.51)\n     - Top countries: United States ($4,560.67), Spain ($4,121.89), France ($3,325.40)\n     - Anomalies: 0 (no anomalies detected)\n     - Day-over-day change: 15.91%\n     - Week-over-week change: 25.88%\n\n5) Follow-up Answers:\n    - If it is a follow-up question from the user within the same session, that is answered by the follow-up agent, only output the follow-up agent's response.\n    - Do not include the daily report in the follow-up response; only the follow-up agent's answer.\n\n6) Output constraints:\n   - Return only the final report text (markdown-friendly). No extra metadata, no JSON, no code blocks.\n   - If any step fails, return the single-line error messages specified above and nothing else.\n\n7) Data fidelity:\n   - Do not modify, infer, or compute metrics yourself. Use only the `metrics` returned by the daily_report_agent.\n   - If values are missing, show \"data not available\" for that field in the Key metrics block.\n\n8) Keep responses concise and manager-focused.\n\nTools available to you:\n- AgentTool(daily_report_agent)\n- AgentTool(summary_agent)\n- AgentTool(followup_agent)\n\nUse them in the exact sequence described.\n\"\"\",\n    tools=[\n        AgentTool(agent=daily_report_agent),\n        AgentTool(agent=summary_agent),\n        AgentTool(agent=followup_agent),\n    ],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:30.222507Z","iopub.execute_input":"2025-11-24T18:39:30.222803Z","iopub.status.idle":"2025-11-24T18:39:30.249655Z","shell.execute_reply.started":"2025-11-24T18:39:30.222782Z","shell.execute_reply":"2025-11-24T18:39:30.248385Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Agent Discussion Interface\n\n - Here the agent provides its responses to the prompt its given.","metadata":{}},{"cell_type":"code","source":"runner = Runner(agent=orchestrator_agent, app_name=APP_NAME, session_service=session_service)\n\nprint(\"âœ… Stateful agent initialized!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:30.250742Z","iopub.execute_input":"2025-11-24T18:39:30.251093Z","iopub.status.idle":"2025-11-24T18:39:30.276236Z","shell.execute_reply.started":"2025-11-24T18:39:30.251063Z","shell.execute_reply":"2025-11-24T18:39:30.275101Z"}},"outputs":[{"name":"stdout","text":"âœ… Stateful agent initialized!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"await run_session(\n    runner,\n    [\n        \"What's my report for today on 16th November 2022?\",\n        \"What actions would you recommend to improve our week-over-week growth?\"\n    ],\n    \"agentic-session-01\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:39:30.277746Z","iopub.execute_input":"2025-11-24T18:39:30.278174Z","iopub.status.idle":"2025-11-24T18:39:40.143727Z","shell.execute_reply.started":"2025-11-24T18:39:30.278131Z","shell.execute_reply":"2025-11-24T18:39:40.142688Z"}},"outputs":[{"name":"stdout","text":"\n ### Session: agentic-session-01\n\nUser > What's my report for today on 16th November 2022?\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\nWARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\nWARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"gemini-2.5-flash-lite >  On 2022-11-16, total revenue reached $21,704.63 with 24 orders, resulting in an average order value of $904.36. The In-store channel was the highest performer, generating $12,694.25, while Furniture led as the top category with $8,980.44 in sales. Sweden, the United States, and the United Kingdom were the top-performing countries, contributing $5,192.24, $4,316.90, and $2,443.86, respectively. There were no anomalies detected. Revenue saw a 5.75% increase compared to the previous day, but it was 9.60% lower than the previous week's average.\n\nKey metrics:\n- Date: 2022-11-16\n- Total revenue: $21,704.63\n- Orders: 24\n- AOV: $904.36\n- Top category: Furniture ($8,980.44)\n- Top channel: In-store ($12,694.25)\n- Top countries: Sweden ($5,192.24), United States ($4,316.90), United Kingdom ($2,443.86)\n- Anomalies: 0 (no anomalies detected)\n- Day-over-day change: 5.75%\n- Week-over-week change: -9.60%\n\nUser > What actions would you recommend to improve our week-over-week growth?\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"gemini-2.5-flash-lite >  To recommend actions, I need to understand the current growth trends. Could you please specify which metrics or areas you'd like to focus on for week-over-week growth analysis? For instance, we could examine overall revenue, sales by product category, or customer acquisition.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"On 2022-11-16, total revenue reached $21,704.63 with 24 orders, resulting in an average order value of $904.36. The In-store channel was the highest performer, generating $12,694.25, while Furniture led as the top category with $8,980.44 in sales. Sweden, the United States, and the United Kingdom were the top-performing countries, contributing $5,192.24, $4,316.90, and $2,443.86, respectively. There were no anomalies detected. Revenue saw a 5.75% increase compared to the previous day, but it was 9.60% lower than the previous week's average.\n\nKey metrics:\n- Date: 2022-11-16\n- Total revenue: $21,704.63\n- Orders: 24\n- AOV: $904.36\n- Top category: Furniture ($8,980.44)\n- Top channel: In-store ($12,694.25)\n- Top countries: Sweden ($5,192.24), United States ($4,316.90), United Kingdom ($2,443.86)\n- Anomalies: 0 (no anomalies detected)\n- Day-over-day change: 5.75%\n- Week-over-week change: -9.60%\n","metadata":{}},{"cell_type":"markdown","source":"To recommend actions, I need to understand the current growth trends. Could you please specify which metrics or areas you'd like to focus on for week-over-week growth analysis? For instance, we could examine overall revenue, sales by product category, or customer acquisition.","metadata":{}}]}